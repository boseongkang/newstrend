name: trend-site

on:
  workflow_dispatch:
    inputs:
      lookback_days:
        description: "How many recent days to include"
        required: false
        default: "30"
  schedule:
    - cron: "15 7 * * *"

permissions:
  contents: read
  actions: read
  pages: write
  id-token: write

concurrency:
  group: trend-site
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install -e .

      - name: Download latest warehouse artifact
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: update-warehouse.yml
          workflow_conclusion: success
          branch: main
          path: artifacts
          if_no_artifact_found: warn

      - name: Restore warehouse
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/warehouse
          WDIR=$(find artifacts -type d -path "*/data/warehouse" | head -1 || true)
          if [ -n "${WDIR:-}" ]; then
            rsync -av "$WDIR/" "data/warehouse/"
          else
            DDIR=$(find artifacts -type d -name data | head -1 || true)
            if [ -n "${DDIR:-}" ]; then
              rsync -av "$DDIR/warehouse/" "data/warehouse/" || true
            fi
          fi
          echo "::group::Restored warehouse tree"
          ls -Rla data/warehouse || true
          echo "::endgroup::"

      - name: Resolve START/END from daily jsonl
        id: rng
        shell: bash
        env:
          LOOKBACK: ${{ inputs.lookback_days }}
        run: |
          set -euo pipefail
          python - <<'PY' > rng.out
          import glob, os, sys, os.path
          n = int(os.getenv("LOOKBACK") or "30")
          fs = sorted(glob.glob("data/warehouse/daily/*.jsonl"))
          if not fs:
              sys.exit(0)
          end = os.path.basename(fs[-1])[:-6]
          start = os.path.basename(fs[-min(n, len(fs))])[:-6]
          print(start)
          print(end)
          PY
          if [ -s rng.out ]; then
            START=$(sed -n '1p' rng.out)
            END=$(sed -n '2p' rng.out)
            echo "START=$START" >> "$GITHUB_OUTPUT"
            echo "END=$END" >> "$GITHUB_OUTPUT"
          fi

      - name: Aggregate and rising
        if: ${{ steps.rng.outputs.START != '' }}
        shell: bash
        run: |
          set -euo pipefail
          RUN="site/${{ steps.rng.outputs.START }}_${{ steps.rng.outputs.END }}"
          mkdir -p "$RUN"
          if [ -f data/warehouse/master.jsonl ]; then
            python scripts/aggregate.py \
              --master data/warehouse/master.jsonl \
              --outdir "$RUN/aggregate" \
              --start "${{ steps.rng.outputs.START }}" \
              --end   "${{ steps.rng.outputs.END }}" \
              --weights   config/publisher_weights.json \
              --blacklist config/publisher_blacklist.txt \
              --extra_stop config/extra_noise.txt \
              --daily-cap 500
          else
            python scripts/aggregate.py \
              --root data/warehouse/daily \
              --pattern "*.jsonl" \
              --outdir "$RUN/aggregate" \
              --start "${{ steps.rng.outputs.START }}" \
              --end   "${{ steps.rng.outputs.END }}" \
              --weights   config/publisher_weights.json \
              --blacklist config/publisher_blacklist.txt \
              --extra_stop config/extra_noise.txt \
              --daily-cap 500
          fi
          python scripts/filter_tokens_csv.py \
            --in "$RUN/aggregate/tokens_by_day.csv" \
            --stop-file config/extra_noise.txt \
            --min-len 4 \
            --out "$RUN/tokens_by_day.cleaned.csv"
          python scripts/rising_from_tokens_csv.py \
            --tokens-csv "$RUN/tokens_by_day.cleaned.csv" \
            --start "${{ steps.rng.outputs.START }}" \
            --end   "${{ steps.rng.outputs.END }}" \
            --window 7 --min-total 20 --topk 500 \
            --outdir "$RUN/rising_csv"
          echo "RUN=$RUN" >> "$GITHUB_OUTPUT"

      - name: Join prices
        if: ${{ steps.rng.outputs.START != '' }}
        shell: bash
        run: |
          set -euo pipefail
          RUN="${{ steps.aggregate_and_rising.outputs.RUN || format('site/{0}_{1}', steps.rng.outputs.START, steps.rng.outputs.END) }}"
          mkdir -p "$RUN/prices_join"
          python scripts/join_prices.py \
            --terms "$RUN/aggregate/tokens_by_day.csv" \
            --map   config/ticker_aliases.json \
            --start "${{ steps.rng.outputs.START }}" \
            --end   "${{ steps.rng.outputs.END }}" \
            --out   "$RUN/prices_join" || true

      - name: Build site bundle
        if: ${{ steps.rng.outputs.START != '' }}
        shell: bash
        run: |
          set -euo pipefail
          SITE="public"
          RUN="site/${{ steps.rng.outputs.START }}_${{ steps.rng.outputs.END }}"
          mkdir -p "$SITE"
          rsync -av "$RUN/" "$SITE/"

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

      - name: Deploy to GitHub Pages
        id: deploy
        uses: actions/deploy-pages@v4
