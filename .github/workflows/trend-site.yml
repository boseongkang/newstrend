name: trend-site

on:
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - run: pip install -r requirements.txt

      - name: Download latest warehouse artifact
        uses: dawidd6/action-download-artifact@v3
        with:
          workflow: update-warehouse.yml
          name: warehouse
          path: artifacts
          allow_forks: true

      - name: Restore warehouse
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/warehouse/daily data/metrics
          if [ -f artifacts/warehouse.zip ]; then unzip -o artifacts/warehouse.zip -d artifacts; fi
          ROOT=""
          for cand in artifacts/data/warehouse artifacts/warehouse artifacts; do
            if [ -f "$cand/master.jsonl" ] || [ -f "$cand/master.jsonl.gz" ] || ls "$cand"/daily/*.jsonl >/dev/null 2>&1; then
              ROOT="$cand"; break
            fi
          done
          if [ -z "$ROOT" ]; then
            HIT=$(find artifacts -type f \( -name master.jsonl -o -name master.jsonl.gz -o -name '*.jsonl' \) | head -n1 || true)
            if [ -n "$HIT" ]; then ROOT=$(dirname "$HIT"); fi
          fi
          if [ -z "$ROOT" ]; then echo "warehouse content not found"; ls -R artifacts; exit 1; fi
          rsync -av "$ROOT/" data/warehouse/
          if [ -d artifacts/data/metrics ]; then rsync -av artifacts/data/metrics/ data/metrics/; fi
          if [ ! -f data/warehouse/master.jsonl ] && ! ls data/warehouse/daily/*.jsonl >/dev/null 2>&1; then
            echo "warehouse files missing after restore"; exit 1
          fi

      - name: Aggregate and rising
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import os, json, glob, re
          from pathlib import Path
          import pandas as pd
          import numpy as np

          run = Path("run"); (run/"aggregate").mkdir(parents=True, exist_ok=True); (run/"rising_csv").mkdir(parents=True, exist_ok=True)

          daily = sorted(glob.glob("data/warehouse/daily/*.jsonl"))
          master = "data/warehouse/master.jsonl"
          files = daily if daily else ([master] if Path(master).exists() else [])
          if not files:
              Path(run/"aggregate"/"articles_by_day.csv").write_text("date,articles\n", encoding="utf-8")
              Path(run/"tokens_by_day.cleaned.csv").write_text("date,term,count\n", encoding="utf-8")
              Path(run/"rising_csv"/"rising_terms_top.csv").write_text("term,norm_slope,last_count,mean\n", encoding="utf-8")
              raise SystemExit("no documents found")

          stop = set("""
          a an and are as at be by for from has have he her his i in is it its of on or our she that the their them they this to was were will with you your not into over under more most less very just than then when what which while where who why how after before during against among between without within about above below across again further few other same any each own both
          """.split())

          rx = re.compile(r"[^a-z ]+")
          counts = {}
          docs = {}
          def add_count(day, term, n=1):
              key = (day, term)
              counts[key] = counts.get(key, 0) + n

          for fp in files:
              with open(fp, "r", encoding="utf-8") as f:
                  for line in f:
                      try:
                          d = json.loads(line)
                      except Exception:
                          continue
                      ts = None
                      for k in ("published_at","publishedAt","date","datetime","created_at","createdAt"):
                          if k in d and d[k]:
                              ts = pd.to_datetime(d[k], errors="coerce"); break
                      if ts is None or pd.isna(ts): 
                          continue
                      day = str(pd.to_datetime(ts).tz_localize(None).date())
                      docs[day] = docs.get(day, 0) + 1
                      text = " ".join(str(d.get(k,"")) for k in ("title","description","content"))
                      text = rx.sub(" ", text.lower())
                      for tok in text.split():
                          if len(tok) < 3 or tok in stop: 
                              continue
                          add_count(day, tok, 1)

          rows = [{"date": d, "term": t, "count": c} for (d,t), c in counts.items()]
          df = pd.DataFrame(rows)
          if df.empty:
              Path(run/"tokens_by_day.cleaned.csv").write_text("date,term,count\n", encoding="utf-8")
          else:
              df = df.groupby(["date","term"])["count"].sum().reset_index().sort_values(["date","count"], ascending=[True, False])
              df.to_csv(run/"tokens_by_day.cleaned.csv", index=False)

          ad = pd.DataFrame({"date": list(docs.keys()), "articles": list(docs.values())})
          ad["date"] = pd.to_datetime(ad["date"]).dt.tz_localize(None)
          ad = ad.sort_values("date")
          ad["date"] = ad["date"].dt.strftime("%Y-%m-%d")
          ad.to_csv(run/"aggregate"/"articles_by_day.csv", index=False)

          def safe_slope(s):
              y = pd.to_numeric(s, errors="coerce").astype(float).to_numpy()
              x = np.arange(len(y), dtype=float)
              msk = np.isfinite(y)
              y = y[msk]; x = x[msk]
              if x.size < 2: return 0.0
              try:
                  m = np.polyfit(x, y, 1)[0]
                  return float(m) if np.isfinite(m) else 0.0
              except Exception:
                  return 0.0

          if not df.empty:
              grid = df.pivot(index="date", columns="term", values="count").sort_index().fillna(0)
              slopes = {c: safe_slope(grid[c]) for c in grid.columns}
              out = pd.DataFrame({"term": list(slopes.keys()), "norm_slope": list(slopes.values())}).sort_values("norm_slope", ascending=False)
              last = grid.iloc[-1] if len(grid) else pd.Series(dtype=float)
              out["last_count"] = out["term"].map(lambda t: int(last.get(t, 0)))
              out["mean"] = out["term"].map(lambda t: float(grid[t].mean()) if t in grid else 0.0)
              out.head(100).to_csv(run/"rising_csv"/"rising_terms_top.csv", index=False)
          else:
              Path(run/"rising_csv"/"rising_terms_top.csv").write_text("term,norm_slope,last_count,mean\n", encoding="utf-8")
          PY

      - name: Build static site
        shell: bash
        run: |
          set -euo pipefail
          python scripts/build_static_ui.py --run run --out site
          echo "GITHUB_RUN_ID=${{ github.run_id }}" >> site/.env

      - name: Upload pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    steps:
      - uses: actions/deploy-pages@v4
