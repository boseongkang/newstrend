name: trend-site

on:
  workflow_dispatch:
    inputs:
      lookback_days:
        description: "How many recent days to include"
        default: "30"
        required: true
  schedule:
    - cron: "12 7 * * *"

permissions:
  contents: read
  actions: read
  pages: write
  id-token: write

concurrency:
  group: trend-site
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install -e .

      - name: Download latest warehouse artifact
        id: dl
        uses: dawidd6/action-download-artifact@v3
        with:
          repo: ${{ github.repository }}
          name: warehouse
          branch: main
          workflow_conclusion: success
          search_artifacts: true
          if_no_artifact_found: warn
          path: artifacts

      - name: Check downloaded artifact
        id: chk
        shell: bash
        run: |
          set -euo pipefail
          cnt=$(find artifacts -type f 2>/dev/null | wc -l | tr -d ' ')
          echo "count=${cnt}" >> "$GITHUB_OUTPUT"
          if [ "$cnt" = "0" ]; then
            echo "::error::No warehouse artifact available yet (try rerun after update-warehouse uploads one)."
            exit 1
          fi          

      - name: Restore warehouse
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p data/warehouse
          if [ -d artifacts/warehouse/data/warehouse ]; then
            rsync -av artifacts/warehouse/data/warehouse/ data/warehouse/
          elif [ -d artifacts/warehouse ]; then
            rsync -av artifacts/warehouse/ data/warehouse/
          else
            echo "warehouse artifact payload not found"; exit 1
          fi
          test -f data/warehouse/master.jsonl

      - name: Resolve START/END from daily jsonl
        id: rng
        env:
          LOOKBACK: ${{ inputs.lookback_days }}
        shell: bash
        run: |
          set -euo pipefail
          mapfile -t FS < <(ls -1 data/warehouse/daily/*.jsonl 2>/dev/null | sort)
          if [ ${#FS[@]} -eq 0 ]; then echo "no daily files"; exit 1; fi
          N="${LOOKBACK:-30}"
          LEN=${#FS[@]}
          if [ "$LEN" -gt "$N" ]; then IDX=$((LEN-N)); else IDX=0; fi
          START=$(basename "${FS[$IDX]}" .jsonl)
          END=$(basename "${FS[$((LEN-1))]}" .jsonl)
          echo "start=$START" >> "$GITHUB_OUTPUT"
          echo "end=$END" >> "$GITHUB_OUTPUT"

      - name: Aggregate and rising
        shell: bash
        run: |
          set -euo pipefail
          OUT="reports/auto_trends_existing/${GITHUB_RUN_ID}"
          python scripts/aggregate.py \
            --master data/warehouse/master.jsonl \
            --outdir "$OUT/aggregate" \
            --start "${{ steps.rng.outputs.start }}" \
            --end   "${{ steps.rng.outputs.end }}" \
            --weights   config/publisher_weights.json \
            --blacklist config/publisher_blacklist.txt \
            --extra_stop config/extra_noise.txt \
            --daily-cap 500
          python scripts/filter_tokens_csv.py \
            --in "$OUT/aggregate/tokens_by_day.csv" \
            --stop-file config/extra_noise.txt \
            --min-len 4 \
            --out "$OUT/tokens_by_day.cleaned.csv"
          python scripts/rising_from_tokens_csv.py \
            --tokens-csv "$OUT/tokens_by_day.cleaned.csv" \
            --start "${{ steps.rng.outputs.start }}" \
            --end   "${{ steps.rng.outputs.end }}" \
            --window 7 --min-total 50 --topk 200 \
            --outdir "$OUT/rising_csv"

      - name: Join prices
        shell: bash
        run: |
          set -euo pipefail
          OUT="reports/auto_trends_existing/${GITHUB_RUN_ID}"
          python scripts/join_prices.py \
            --terms "$OUT/aggregate/tokens_by_day.csv" \
            --map   config/ticker_aliases.json \
            --start "${{ steps.rng.outputs.start }}" \
            --end   "${{ steps.rng.outputs.end }}" \
            --out   "$OUT/prices_join"

      - name: Build site bundle
        shell: bash
        run: |
          set -euo pipefail
          RUNID="${GITHUB_RUN_ID}"
          SRC="reports/auto_trends_existing/${RUNID}"
          DEST="site"
          mkdir -p "$DEST/run"
          rsync -av "$SRC"/ "$DEST/run/" || true
          mkdir -p "$DEST"
          {
            echo "<meta charset='utf-8'><h1>News Trends</h1><ul>"
            find "$DEST/run" -type f | sed 's#^site/##' | sort | while read -r p; do
              printf "<li><a href='%s'>%s</a></li>\n" "$p" "$p"
            done
            echo "</ul>"
          } > "$DEST/index.html"

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    needs: build
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
