name: trend-site

on:
  workflow_dispatch:
  schedule:
    - cron: "25 7 * * *"
  push:
    branches: [ main ]
    paths:
      - "scripts/**"
      - "site/**"
      - "config/**"
      - "configs/**"
      - ".github/workflows/trend-site.yml"

permissions:
  actions: read
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install -U pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt || true; fi
          pip install yfinance pandas numpy orjson pyyaml || true

      - name: Get latest warehouse run id
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          RID=$(gh run list --workflow update-warehouse.yml --branch main --status success --json databaseId -q '.[0].databaseId')
          echo "RUN_ID=$RID" >> $GITHUB_ENV

      - name: Download warehouse artifact
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p artifacts
          gh run download "$RUN_ID" -n warehouse -D artifacts

      - name: Extract warehouse
        run: |
          set -euo pipefail
          mkdir -p data/warehouse
          if ls artifacts/*.zip >/dev/null 2>&1; then unzip -o artifacts/*.zip -d artifacts >/dev/null; fi
          if [ -f artifacts/artifact.tar ]; then tar -xf artifacts/artifact.tar -C artifacts; fi
          if [ -d artifacts/warehouse ]; then
            rsync -a artifacts/warehouse/ data/warehouse/
          elif [ -d artifacts/daily ]; then
            mkdir -p data/warehouse/daily
            rsync -a artifacts/daily/ data/warehouse/daily/
          elif [ -d artifacts/artifacts ]; then
            rsync -a artifacts/artifacts/ data/
          fi
          test -d data/warehouse/daily
          echo "extracted daily:" && ls -lt data/warehouse/daily | head -n 5 || true

      - name: Convert csv -> legacy *_tokens.jsonl
        run: |
          python - <<'PY'
          import pandas as pd, json, pathlib
          p = pathlib.Path("data/warehouse/daily")
          for f in sorted(p.glob("*.csv")):
              dt = f.stem
              df = pd.read_csv(f)
              j = p / f"{dt}_tokens.jsonl"
              with j.open("w", encoding="utf-8") as w:
                  for r in df.itertuples(index=False):
                      w.write(json.dumps({"tok": str(r.entity), "n": int(r.count)}, ensure_ascii=False) + "\n")
          print("csv -> *_tokens.jsonl done")
          PY
          ls -lt data/warehouse/daily | head -n 8 || true

      - name: Aggregate from warehouse
        run: |
          python scripts/aggregate_from_warehouse.py \
            --warehouse data/warehouse/daily \
            --out run \
            --last-days 30 \
            --min-len 4 \
            --extra-stop config/extra_noise.txt

      - name: Build static pages
        run: |
          python scripts/build_static_ui.py --run run --out site
          test -s site/index.html
          test -s site/report.html
          test -s site/rising.html

      - name: Make prices.json
        run: |
          python - <<'PY'
          import json, pathlib
          import pandas as pd, yfinance as yf
          m = json.loads(pathlib.Path("config/ticker_aliases.json").read_text(encoding="utf-8"))
          tickers = sorted(m.keys())
          today = pd.Timestamp.today().date()
          end = pd.Timestamp(today)
          if end.dayofweek >= 5:
            end = end - pd.tseries.offsets.BDay(1)
          start = (end - pd.tseries.offsets.BDay(89)).date().isoformat()
          endp1 = (end + pd.Timedelta(days=1)).strftime("%Y-%m-%d")
          df = yf.download(tickers, start=start, end=endp1, group_by="ticker", progress=False, threads=True, interval="1d")
          if isinstance(df.columns, pd.MultiIndex):
              dates = [pd.Timestamp(d).date().isoformat() for d in df.index]
              close = {}
              top = set(df.columns.get_level_values(0))
              for t in tickers:
                  if t in top:
                      s = df[t]["Close"]
                      close[t] = [None if pd.isna(v) else float(v) for v in s.tolist()]
                  else:
                      close[t] = [None]*len(dates)
          else:
              dates = [pd.Timestamp(d).date().isoformat() for d in df.index]
              close = {tickers[0]: [None if pd.isna(v) else float(v) for v in df["Close"].tolist()]}
          out = {"dates": dates, "tickers": tickers, "close": close}
          p = pathlib.Path("site/data"); p.mkdir(parents=True, exist_ok=True)
          (p/"prices.json").write_text(json.dumps(out, indent=2, ensure_ascii=False))
          PY
          test -s site/data/prices.json

      - name: Copy daily files into site (after build)
        run: |
          set -euo pipefail
          mkdir -p site/data/daily
          rsync -a data/warehouse/daily/*.jsonl site/data/daily/ 2>/dev/null || true
          rsync -a data/warehouse/daily/*_tokens.jsonl site/data/daily/ 2>/dev/null || true
          rsync -a data/warehouse/daily/*.jsonl.gz site/data/daily/ 2>/dev/null || true
          echo "site daily files:" && ls -lt site/data/daily | head -n 10 || true

      - name: Cache-bust data urls
        run: |
          SHA="${GITHUB_SHA}"
          find site -type f \( -name "*.html" -o -name "*.js" \) -print0 \
            | xargs -0 sed -i.bak -E "s#(data/[^\"']+\.(jsonl|json)(\.gz)?)#\1?v=${SHA}#g"
          find site -name "*.bak" -delete

      - name: Assert today's file exists in site
        run: |
          D=$(date -u +%F)
          ls -l site/data/daily | tail -n 20 || true
          test -s "site/data/daily/${D}_tokens.jsonl" || \
          test -s "site/data/daily/${D}.jsonl" || \
          test -s "site/data/daily/${D}.jsonl.gz" || \
          (echo "Missing today's daily file in site/data/daily" && exit 1)

      - uses: actions/configure-pages@v5

      - uses: actions/upload-pages-artifact@v3
        with:
          path: site

      - uses: actions/deploy-pages@v4