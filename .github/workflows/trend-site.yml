name: trend-site

on:
  workflow_dispatch:
  schedule:
    - cron: "25 7 * * *"
  push:
    branches: [ main ]
    paths:
      - "scripts/**"
      - "site/**"
      - "config/**"
      - ".github/workflows/trend-site.yml"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages"
  cancel-in-progress: true

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install orjson

      - name: Get latest warehouse run id
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          RID=$(gh run list --workflow update-warehouse.yml --branch main --status success --json databaseId -q '.[0].databaseId')
          echo "RUN_ID=$RID" >> $GITHUB_ENV

      - name: Download warehouse artifact
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p artifacts
          gh run download "$RUN_ID" -n warehouse -D artifacts

      - name: Extract warehouse
        run: |
          set -euo pipefail
          mkdir -p data/warehouse
          if ls artifacts/*.zip >/dev/null 2>&1; then unzip -o artifacts/*.zip -d artifacts >/dev/null; fi
          if [ -f artifacts/artifact.tar ]; then tar -xf artifacts/artifact.tar -C artifacts; fi
          if [ -d artifacts/warehouse ]; then
            rsync -a artifacts/warehouse/ data/warehouse/
          elif [ -d artifacts/daily ]; then
            mkdir -p data/warehouse/daily
            rsync -a artifacts/daily/ data/warehouse/daily/
          elif [ -d artifacts/artifacts ]; then
            rsync -a artifacts/artifacts/ data/
          fi
          test -d data/warehouse/daily

      - name: Aggregate from warehouse
        run: |
          python scripts/aggregate_from_warehouse.py --warehouse data/warehouse/daily --out run --last-days 30 --min-len 4 --extra-stop config/extra_noise.txt

      - name: Build site
        run: |
          python scripts/build_static_ui.py --run run --out site
          python - <<'PY'
          import json, pathlib, re, datetime as dt
          from collections import defaultdict, Counter
          import orjson
          site = pathlib.Path("site"); (site/"data").mkdir(parents=True, exist_ok=True)
          rng = pathlib.Path("run")/"date_range.txt"
          if rng.exists():
              s,e = open(rng).read().strip().split()
          else:
              s = min(p.name[:10] for p in pathlib.Path("data/warehouse/daily").glob("*.jsonl"))
              e = max(p.name[:10] for p in pathlib.Path("data/warehouse/daily").glob("*.jsonl"))
          s = dt.date.fromisoformat(s); e = dt.date.fromisoformat(e)
          pubs = Counter()
          day_keys = defaultdict(set)
          for f in sorted(pathlib.Path("data/warehouse/daily").glob("*.jsonl")):
              m = re.match(r"(\d{4}-\d{2}-\d{2})\.jsonl", f.name)
              if not m: continue
              d = dt.date.fromisoformat(m.group(1))
              if d<s or d>e: continue
              with open(f,"rb") as fh:
                  for line in fh:
                      try: o = orjson.loads(line)
                      except: continue
                      v = o.get("publisher") or o.get("source") or o.get("source_name") or o.get("site") or o.get("domain") or ""
                      if isinstance(v, dict): v = v.get("name") or v.get("id") or ""
                      if isinstance(v, list): v = v[0] if v else ""
                      v = str(v).strip()
                      if v: pubs[v]+=1
                      url = (o.get("url") or o.get("link") or "").strip()
                      title = (o.get("title") or "").strip()
                      key = url or (title, v)
                      if key: day_keys[d].add(key)
          top_pubs = pubs.most_common(50)
          (site/'data/publishers.json').write_text(json.dumps({'labels':[k for k,_ in top_pubs],'counts':[int(v) for _,v in top_pubs]}))
          dates, counts = [], []
          d = s
          while d<=e:
              dates.append(d.isoformat())
              counts.append(len(day_keys.get(d,set())))
              d += dt.timedelta(days=1)
          (site/'data/articles.json').write_text(json.dumps({'dates':dates,'articles':counts}))
          PY
          cp scripts/static_dashboard.html site/index.html
          cp scripts/report.html site/report.html
          test -f site/index.html
          test -f site/report.html

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

      - name: Deploy to Pages
        id: deployment
        uses: actions/deploy-pages@v4