name: trend-site
on:
  workflow_dispatch:
    inputs:
      lookback_days:
        description: How many recent days
        required: false
        default: "30"

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: trend-site
  cancel-in-progress: false

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with: {fetch-depth: 0}

      - uses: actions/setup-python@v5
        with: {python-version: "3.12"}

      - name: Install
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          pip install -e .

      - name: Download latest warehouse artifact
        id: dl
        uses: dawidd6/action-download-artifact@v3
        with:
          repo: ${{ github.repository }}
          workflow: update-warehouse.yml
          workflow_conclusion: success
          branch: main
          name: warehouse
          search_artifacts: true
          path: artifacts
          if_no_artifact_found: warn
          github_token: ${{ secrets.GITHUB_TOKEN }}

      - name: Restore warehouse
        id: restore
        run: |
          set -euo pipefail
          mkdir -p data
          BASE="artifacts"
          if [ -d "${BASE}/warehouse" ]; then BASE="${BASE}/warehouse"; fi
          if [ -d "${BASE}" ]; then rsync -a "${BASE}/" ./ || true; fi
          if [ -f data/warehouse/master.jsonl ]; then
            echo "has_master=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_master=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Resolve START/END
        id: rng
        env:
          LOOKBACK: ${{ inputs.lookback_days || '30' }}
        run: |
          set -euo pipefail
          if [ "${{ steps.restore.outputs.has_master }}" != "true" ]; then
            echo "start=" >> "$GITHUB_OUTPUT"
            echo "end="   >> "$GITHUB_OUTPUT"
            exit 0
          fi
          list=$(ls -1 data/warehouse/daily/*.jsonl 2>/dev/null || true)
          if [ -n "$list" ]; then
            START=$(echo "$list" | tail -n "$LOOKBACK" | head -n 1 | sed -E 's#.*/([0-9]{4}-[0-9]{2}-[0-9]{2}).jsonl#\1#')
            END=$(  echo "$list" | tail -n 1                 | sed -E 's#.*/([0-9]{4}-[0-9]{2}-[0-9]{2}).jsonl#\1#')
          else
            START=""
            END=""
          fi
          echo "start=$START" >> "$GITHUB_OUTPUT"
          echo "end=$END"     >> "$GITHUB_OUTPUT"

      - name: Aggregate and rising
        id: agg
        env:
          RUNID: ${{ github.run_id }}
          START: ${{ steps.rng.outputs.start }}
          END:   ${{ steps.rng.outputs.end }}
          LOOKBACK: ${{ inputs.lookback_days || '30' }}
        run: |
          set -euo pipefail
          OUT="reports/auto_trends_existing/${RUNID}"
          mkdir -p "$OUT"
          if [ "${{ steps.restore.outputs.has_master }}" != "true" ]; then
            echo "created_tokens=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          set +e
          if [ -n "${START}" ] && [ -n "${END}" ]; then
            python scripts/aggregate.py --master data/warehouse/master.jsonl --outdir "$OUT/aggregate" --start "${START}" --end "${END}" --weights config/publisher_weights.json --blacklist config/publisher_blacklist.txt --extra_stop config/extra_noise.txt --daily-cap 500
          fi
          rc=$?
          set -e
          if [ ! -f "$OUT/aggregate/tokens_by_day.csv" ]; then
            python scripts/aggregate.py --master data/warehouse/master.jsonl --outdir "$OUT/aggregate" --days "${LOOKBACK}" --weights config/publisher_weights.json --blacklist config/publisher_blacklist.txt --extra_stop config/extra_noise.txt --daily-cap 500 || true
          fi
          if [ -f "$OUT/aggregate/tokens_by_day.csv" ]; then
            python scripts/filter_tokens_csv.py --in "$OUT/aggregate/tokens_by_day.csv" --stop-file config/extra_noise.txt --min-len 4 --out "$OUT/tokens_by_day.cleaned.csv" || true
          fi
          if [ -f "$OUT/tokens_by_day.cleaned.csv" ]; then
            python scripts/rising_from_tokens_csv.py --tokens-csv "$OUT/tokens_by_day.cleaned.csv" --start "${START:-}" --end "${END:-}" --window 7 --min-total 20 --topk 500 --outdir "$OUT/rising_csv" || true
            echo "created_tokens=true" >> "$GITHUB_OUTPUT"
          else
            echo "created_tokens=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Build site bundle
        env:
          RUNID: ${{ github.run_id }}
          CREATED: ${{ steps.agg.outputs.created_tokens }}
        run: |
          set -euo pipefail
          SRC="reports/auto_trends_existing/${RUNID}"
          DEST="site"
          mkdir -p "$DEST/run"
          if [ "${CREATED}" = "true" ] && [ -d "$SRC" ]; then
            rsync -av "$SRC"/ "$DEST/run/"
            printf '<meta charset="utf-8"><h1>News Trends</h1><ul>\n' > "$DEST/index.html"
            find "$DEST/run" -type f | sort | sed -E 's#^site/##' | awk '{printf "<li><a href=\"%s\">%s</a></li>\n",$0,$0}' >> "$DEST/index.html"
            printf '</ul>\n' >> "$DEST/index.html"
          else
            mkdir -p "$DEST"
            printf '<meta charset="utf-8"><h1>No data yet</h1><p>Warehouse artifact missing or no documents in the selected window.</p>\n' > "$DEST/index.html"
          fi

      - name: Upload pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

  deploy:
    needs: build
    runs-on: ubuntu-latest
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - id: deployment
        uses: actions/deploy-pages@v4
