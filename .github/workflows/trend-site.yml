name: trend-site

on:
  workflow_dispatch:
  schedule:
    - cron: "25 7 * * *"
  push:
    branches: [ main ]
    paths:
      - "scripts/**"
      - "site/**"
      - "config/**"
      - "configs/**"
      - ".github/workflows/trend-site.yml"
  workflow_run:
    workflows: ["update-warehouse"]
    types: [completed]

permissions:
  actions: read
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

jobs:
  build:
    if: ${{ github.event_name != 'workflow_run' || github.event.workflow_run.conclusion == 'success' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install deps
        run: |
          python -m pip install -U pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt || true; fi
          pip install yfinance pandas numpy orjson pyyaml || true

      - name: Get latest warehouse run id
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          RID=$(gh run list --workflow update-warehouse.yml --branch main --status success --json databaseId -q '.[0].databaseId')
          echo "RUN_ID=$RID" >> $GITHUB_ENV

      - name: Download warehouse artifact
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          mkdir -p artifacts
          gh run download "$RUN_ID" -n warehouse -D artifacts

      - name: Extract warehouse
        run: |
          set -euo pipefail
          mkdir -p data/warehouse
          if ls artifacts/*.zip >/dev/null 2>&1; then unzip -o artifacts/*.zip -d artifacts >/dev/null; fi
          if [ -f artifacts/artifact.tar ]; then tar -xf artifacts/artifact.tar -C artifacts; fi
          if [ -d artifacts/warehouse ]; then
            rsync -a artifacts/warehouse/ data/warehouse/
          elif [ -d artifacts/daily ]; then
            mkdir -p data/warehouse/daily
            rsync -a artifacts/daily/ data/warehouse/daily/
          elif [ -d artifacts/artifacts ]; then
            rsync -a artifacts/artifacts/ data/
          fi
          test -d data/warehouse/daily
          ls -lt data/warehouse/daily | head -n 8 || true

      - name: Convert csv/jsonl to *_tokens.jsonl
        run: |
          set -euo pipefail
          python scripts/csv_to_tokens.py
          DT=$(ls data/warehouse/daily/*_tokens.jsonl | sed -E 's#.*/([0-9]{4}-[0-9]{2}-[0-9]{2})_tokens\.jsonl#\1#' | sort | tail -n1 || true)
          [ -n "$DT" ] && echo "LATEST_TOKENS=$DT" >> $GITHUB_ENV || true

      - name: Mirror *_tokens.jsonl -> *_tokens.csv
        run: |
          set -euo pipefail
          python scripts/tokens_jsonl_to_csv.py

      - name: Aggregate from warehouse
        run: |
          python scripts/aggregate_from_warehouse.py \
            --warehouse data/warehouse/daily \
            --out run \
            --last-days 7 \
            --min-len 4 \
            --extra-stop config/extra_noise.txt

      - name: Build static pages
        run: |
          python scripts/build_static_ui.py --run run --out site
          test -s site/index.html
          test -s site/report.html
          test -s site/rising.html

      - name: Join prices
        run: |
          python - <<'PY'
          import pandas as pd, os
          df = pd.read_csv("run/tokens_by_day.cleaned.csv")
          s = pd.to_datetime(df['date']).min().date()
          e = pd.to_datetime(df['date']).max().date()
          with open(os.environ["GITHUB_ENV"], "a") as fh:
              fh.write(f"START={s}\n")
              fh.write(f"END={e}\n")
          PY
          python scripts/join_prices.py \
            --terms run/tokens_by_day.cleaned.csv \
            --map   config/ticker_aliases.json \
            --start "$START" \
            --end   "$END" \
            --out   run/prices.csv \
            --min-days 1          

      - name: Export prices.json
        run: |
          python - <<'PY'
          import pandas as pd, json, pathlib, sys
          base = pathlib.Path("run")/"prices.csv"
          td = pd.read_csv(base/"ticker_daily.csv")
          date_col, tick_col = "date", "ticker"
          cand = ["close","Close","Adj Close","adj_close","price","Price"]
          val_col = next((c for c in cand if c in td.columns), None)
          if val_col is None:
              raise SystemExit("no price column found")
          td = td[[date_col, tick_col, val_col]].rename(columns={val_col: "close"})
          dates   = sorted(td[date_col].unique().tolist())
          tickers = sorted(td[tick_col].unique().tolist())
          pv = (td.pivot(index=date_col, columns=tick_col, values="close")
                  .reindex(dates)
                  .reindex(columns=tickers))
          close = {t: [None if pd.isna(v) else float(v) for v in pv[t].tolist()] for t in tickers}
          out = {"dates": dates, "tickers": tickers, "close": close}
          outp = pathlib.Path("site/data"); outp.mkdir(parents=True, exist_ok=True)
          (outp/"prices.json").write_text(json.dumps(out, indent=2, ensure_ascii=False))
          PY

      - name: Build tickers.json (sectors)
        run: |
          python scripts/make_ticker_sectors.py --map config/ticker_aliases.json --out site/data/tickers.json || true

      - name: Copy daily files into site
        run: |
          set -euo pipefail
          mkdir -p site/data/daily
          rsync -a data/warehouse/daily/*.jsonl site/data/daily/ 2>/dev/null || true
          rsync -a data/warehouse/daily/*_tokens.jsonl site/data/daily/ 2>/dev/null || true
          rsync -a data/warehouse/daily/*.jsonl.gz site/data/daily/ 2>/dev/null || true

      - name: Build aggregate entities_daily.jsonl
        run: |
          set -euo pipefail
          python scripts/build_entities_daily.py

      - name: Duplicate aggregate to legacy paths
        run: |
          set -euo pipefail
          cp -f site/data/entities_daily.jsonl site/entities_daily.jsonl || true
          cp -f site/data/entities_daily.jsonl site/data/entities.jsonl || true
          gzip -f -k site/data/entities_daily.jsonl
          gzip -f -k site/entities_daily.jsonl
          gzip -f -k site/data/entities.jsonl

      - name: Determine latest date and export
        run: |
          set -euo pipefail
          LATEST=$(ls site/data/daily/*.jsonl* 2>/dev/null | sed -E 's#.*/([0-9]{4}-[0-9]{2}-[0-9]{2}).*#\1#' | sort | tail -n1 || true)
          if [ -z "${LATEST:-}" ]; then echo "No daily files found in site/data/daily"; exit 1; fi
          echo "LATEST=$LATEST" >> $GITHUB_ENV

      - name: Cache-bust data urls
        run: |
          set -euo pipefail
          SHA="${GITHUB_SHA}"
          D="${LATEST:-unknown}"
          find site -type f -name "*.html" -print0 | xargs -0 --no-run-if-empty sed -i.bak -E "s#(data/[^\"']+\.(jsonl|json)(\.gz)?)#\1?v=${SHA}&d=${D}#g"
          find site -type f -name "*.js" -print0 | xargs -0 --no-run-if-empty sed -i.bak -E "s#(data/[^\"']+\.(jsonl|json)(\.gz)?)#\1?v=${SHA}&d=${D}#g"
          find site -name "*.bak" -delete

      - name: Create .nojekyll
        run: echo > site/.nojekyll

      - name: Upload site debug artifact
        uses: actions/upload-artifact@v4
        with:
          name: site-debug
          path: site

      - uses: actions/configure-pages@v5

      - id: upload
        uses: actions/upload-pages-artifact@v3
        with:
          path: site

      - id: deploy
        uses: actions/deploy-pages@v4